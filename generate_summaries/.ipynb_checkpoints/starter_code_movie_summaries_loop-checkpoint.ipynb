{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses ths guide: https://stackabuse.com/text-summarization-with-nltk-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need all these packages. You might need to install the extra nltk packages (see: https://www.nltk.org/data.html)\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read movie summaries into Pandas\n",
    "# Store Movie ID in first column, summary in second column.\n",
    "# Should make a n x 2 matrix where n = number of movie summaries\n",
    "df = pd.read_csv('plot_summaries_short.txt',sep='\\t', names = [\"ID\", \"summary\"])\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Square Brackets and Extra Spaces\n",
    "\n",
    "size = len(df.summary)\n",
    "article_text = [\"\" for x in range(size)]\n",
    "\n",
    "for i in range(size):\n",
    "    article_text[i] = re.sub(r'\\[[0-9]*\\]', ' ', df.summary[i])\n",
    "    article_text[i] = re.sub(r'\\s+', ' ', df.summary[i])\n",
    "\n",
    "\n",
    "# QC. The list should be as long as the total number of rows in the Pandas dataframe\n",
    "print('The article_text list contains {0} article summaries.'.format(len(article_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC. Slice the first 6 elements in the article_text list. Make sure that they are cleaned correctly and\n",
    "# Make sure that one summary is still one element in the list\n",
    "\n",
    "article_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters and digits\n",
    "\n",
    "formatted_article_text = [\"\" for x in range(size)]\n",
    "for i in range(size):\n",
    "    formatted_article_text[i] = re.sub('[^a-zA-Z]', ' ', article_text[i])\n",
    "    formatted_article_text[i] = re.sub(r'\\s+', ' ', formatted_article_text[i])\n",
    "\n",
    "# QC. The list should be as long as the total number of rows in the Pandas dataframe\n",
    "print('The formatted_article_text list contains {0} article summaries.'.format(len(formatted_article_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC. Slice the first 6 elements in the article_text list. Make sure that they are cleaned correctly and\n",
    "# Make sure that one summary is still one element in the list\n",
    "\n",
    "formatted_article_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Text to Sentences\n",
    "\n",
    "sentence_list = [[] for x in range(size)]\n",
    "for i in range(size):\n",
    "    sentence_list[i] = nltk.sent_tokenize(article_text[i])\n",
    "\n",
    "# QC. The list should be as long as the total number of rows in the Pandas dataframe\n",
    "print('The sentence_list is {0} rows long.'.format(len(sentence_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC. Slice the first 6 elements in the sentence_text list. Make sure that they are cleaned correctly and\n",
    "# Make sure that one summary is still one element in the list\n",
    "\n",
    "sentence_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For QCing the word frequency loop. It calculates word frequencies for the 1st element in the list of summaries\n",
    "\n",
    "# Find Weighted Frequency of Occurance\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies_test = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text[1]):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies_test.keys():\n",
    "            word_frequencies_test[word] = 1\n",
    "        else:\n",
    "            word_frequencies_test[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_frequencies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Weighted Frequency of Occurance\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {x:{} for x in range(size)}\n",
    "\n",
    "for i in range(size):\n",
    "    for word in nltk.word_tokenize(formatted_article_text[i]):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies[i].keys():\n",
    "                word_frequencies[i][word] = 1\n",
    "            else:\n",
    "                word_frequencies[i][word] += 1\n",
    "\n",
    "# QC. The list should be as long as the total number of rows in the Pandas dataframe\n",
    "print('The word_frequencies dictionary is {0} rows long.'.format(len(word_frequencies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# QC. Index the 1st element in the dictionary. The frequency counts should match word_frequencies_test\n",
    "print(word_frequencies[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Max Frequency\n",
    "\n",
    "maximum_frequncy = [\"\" for x in range(size)]\n",
    "\n",
    "for i in range(size):\n",
    "    maximum_frequncy[i] = max(word_frequencies[i].values())\n",
    "    \n",
    "# QC. The maximum_frequncy list should be as long as the total number of rows in the Pandas dataframe\n",
    "print('The maximum_frequncy list is {0} rows long.'.format(len(maximum_frequncy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC to check that this list contains the max word frequency for each plot summary\n",
    "\n",
    "print(maximum_frequncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Weighted Frequency\n",
    "\n",
    "for i in range(size):\n",
    "    for word in word_frequencies[i].keys():\n",
    "        word_frequencies[i][word] = (word_frequencies[i][word]/maximum_frequncy[i])\n",
    "        \n",
    "# QC. The maximum_frequncy list should be as long as the total number of rows in the Pandas dataframe\n",
    "print('The word_frequncies list is {0} rows long.'.format(len(word_frequencies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC to check that each word contains its weighted frequency: the word's count divided by the max word count\n",
    "\n",
    "print(word_frequencies[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Sentence Scores\n",
    "\n",
    "sentence_scores = {x:{} for x in range(size)}\n",
    "\n",
    "for i in range(size):\n",
    "    for sent in sentence_list[i]:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies[i].keys():\n",
    "                if len(sent.split(' ')) < 20: ## Change this to specify how long/short of sentences you want to include\n",
    "                    if sent not in sentence_scores[i].keys():\n",
    "                        sentence_scores[sent] = word_frequencies[i][word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencie[i][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Summary\n",
    "# This needs to be better. Instead of summarizing the text defined above, this should be a for loop that\n",
    "# Runs a text summary on every row in the pandas data frame defined above\n",
    "\n",
    "\n",
    "#Change value here to get summary sentence length\n",
    "summary_sentences = heapq.nlargest(2, sentence_scores[1], key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, now it is time to score the summaries on arousal, valence, and dominance. There is a nice dictionary by\n",
    "# Bradley and Lang that will let you do exactly that. You can borrow that here:\n",
    "# https://github.com/dwzhou/SentimentAnalysis\n",
    "\n",
    "# Biasically, you'll want to classify each cell in the Pandas Dataframe (where each cell contains a movie summary)\n",
    "# along arousal, valence, and dominance. So we can get a score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is super optional and likely not necissary. You could, instead of using a dictionary approach,\n",
    "# Train a classifier to do your sentiment analysis. This would be cooler, but also probably a lot of work\n",
    "# And I'm not sure it would gain us all that much. But if you are feeling energetic, or the Lang dictionary\n",
    "# doesn't work, here is some hints about training a classifier.\n",
    "\n",
    "#Train a text sentiment classifier. Here we are in good shape because most are trained on movie ratings\n",
    "# But you could also train the classifier on the un summarized movie reviews\n",
    "# For ideas on how to do this, check out: https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
